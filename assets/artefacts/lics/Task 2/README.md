# Readme

Task 2 was to make the planned application from Task 1, using Python, with an accompanying readme file, that was upto 800 words long.

## Feedback

The rubrik was presented as follows:

| Component | Contribution (%) | My Score | Feedback (+/-) |
| --------- | ---------------- | -------- | ------- |
| Knowledge and Understanding | 30% | No score | + "An excellenet demonstration of knowledge and understanding, displaying some originality and in-depth understanding in all key areas of knowledge relevant to work" <br> - "You have used too many options for the users, which could have been avoided to make the application short and simple." |
| Application of Knowledge and Understanding | 30% | No score | + "An excellent demonstration of the application of knowledge and understanding to address the learning outcomes assessed by the assignment and/or application to professional practise or real-world applicability." <br> + "The GUI has been produced exceptionally well with full functionalities." <br> - "There are too many options evident on the terminal, which is a negative aspect." |  
| Structure and Presentation | 30% | No score | + "Good structure and presentation, which shows elements of presentation and structure at a professional standard." <br> + "The presentation of testing and validation look amazing with required evidences and information." <br> - "You could have paid more focus on placing all figures in the centre of the page with more information." <br> - "The presentation of the main algorithm and Python script part could have been better."|
| Academic Intregrity | 10% | No score | + "A very good demonstration of academic writing, including use of academic conventions, citations and/or referencing with no errors." <br> + "Work shows broadly accurate attempts" <br> + "You have used few sources with accuracy."|

In total, my score was 77%. I was never given a percentage breakdown of my score. However, I was given additional comments:

| Component | Feedback (+/-) |
| --------- | -------------- |
| Overall Comments | + "Overall an excellent effort" <br> + "Well-documented assigment with examples and test cases" <br> + "Good logic using for and if loop" <br> + "comments are used to explain and outputs are shown" <br> + "Brilliant use of screenshots to demonstrate the functionality" <br> + "Your implementation and discussions are excellent" <br> + "The report is organised in a brilliant way with detailed explanations" <br> + "Appropiate code has been used in terms of implmentation and use of functionalities, e.g use of object-oriented programming concepts, or functional aspects, but in general you did a brilliant work." <br> + "List positioning using list indexing has been used correctly" <br> + "The test results are correct" <br> + "Overall the coding is very good in terms of libraries and Python data structures" <br> + "Code implments the correct logic and good use of for and if" <br> + "Variables are defined correctly" <br> + "Very good use of visualisations in the report" <br> - "The report could be further extended to include more discussion and analysis." <br> - "could have discussed for implementation and instruction of how to use" <br> - "Test strategy could be described in detail"|

## Reflection

Overall, I'm happy with the score, but again, I found alot of the feedback more confusing than explicitly helpful.

Assignment feedback is the only way a student can learn about how to do better next time. I only received a few comments that provided an oppurtunity for me to improve, but my score was only 77%.

### Potential for Improvement

Below are all of the hints I received for improvement, that were pulled from the Task 2 feedback.

* It's claimed that I gave the user too many options. I gave the user around 10 options, to demonstrate all of the functionality that I was instructed to do so. I also stated in the readme file that the final version of the UI would be a VUI, so there wouldn't be a list in that case. Nevertheless, I agree that a CLI application which has many options, is not ideal. I feel like I was forced to do it in this case though.
* It's claimed that I could have placed the figures in the centre of the page with more information. This is not even relevant to Task 2, and is in fact a criticism of my submission for Task 1. Feedback like this is frustrating because to me it displays assessment incompetency. Why didn't I receive that feedback in the Task 1 feedback? Did that affect my score for Task 2?
* It's claimed that the presentation of the main algorithm could have been improved. I talked about this in my readme, pointing out that I was instructed to use python 3.4 so I couldn't use switch/case statements or the walrus operator. That ability would have improved readability for sure, as essentially my main alorithm was just getting user input, cleaning it, interacting with the database, and then responding to the user with some text, relying on a lot of if statements. However, I created functions specifically for getting input and cleaning it, and for database interactions, to simplify my makeshift switch. No doubt though, I could have made higher level functions too; one for for each user option, to help the main algorithm appear simpler.
* It's claimed that my report could have been extended with more discussion and analysis. This is too generic and unclear to be useful. Does a readme file count as a report, or is this referring to my plan from Task 1? What type of analysis was expected? There was no analysis mentioned in the instructions of Task 2.
* It's claimed I didn't discuss instructions or implementations. The first paragraph of my readme contains instructions, and the rest of the readme is related to implementations. A model answer might have been helpful here, if there was some presentation issue.
* It's claimed my test strategy could be described in detail. This is referring to Task 1 again I presume, where I was tasked to plan a test strategy. For my Task 2 readme, I could have given instructions specifically on how to enter debug mode, but I felt that was self explanatory as there is literally an option for it, just like every other functionality. It was my understanding that we could assume some compentency with CLI's in the user.

From this feedback and reflection alone, I'm unsure what I could have done to improve on Task 2, outside of minor changes: function nesting, option nesting, and instructions specifically on how to select the debug mode option amongst others. I don't think these points alone justify the 23% marks I lost for the assignment, given the rubrik; so I find the feedback policy completely unethical. 